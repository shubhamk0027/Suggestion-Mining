{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SequenceGeneratorFastText.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vxpjnqeREvK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#####################################################################################\n",
        "######################### IMPORTING THE LIBRARIES AND DATASET #######################\n",
        "#####################################################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ly7fty8sRabJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# FOR PLOTTING GRAPHS\n",
        "import matplotlib.pyplot as plt\n",
        "pd.set_option('display.max_colwidth',300)\n",
        "\n",
        "# FOR REMOVING SPECIAL CHARACTERS, LINKS, AND EXPANDING WORDS\n",
        "import re\n",
        "import unicodedata\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# FOR STEMMING AND REMOVING STOP WORDS\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer   \n",
        "from nltk.corpus import wordnet\n",
        "from nltk import pos_tag\n",
        "\n",
        "# FOR BUILDING THE EMBEDDING MATRIX AND GENERATING THE SEQUENCES\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "# FOR THE FAST TEST WORD VECTOR WEIGHTS\n",
        "from tqdm import tqdm\n",
        "import codecs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FpPy8R-MVFi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "outputId": "f37ef157-1082-4e01-ba39-bebe6c5445be"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "#### CACHING THE STOP WORDS HELPS IN FASTENING THE REMOVAL OF THE STOP WORDS\n",
        "cachedStopWords = stopwords.words(\"english\")\n",
        "nltk.download('wordnet')\n",
        "nltk.download('words')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "corpus_words = set(nltk.corpus.words.words())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcT_qFibRkHt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# IMPORTING THE DATASET\n",
        "train_data=pd.read_csv(\"TrainingData.csv\")\n",
        "test_data=pd.read_csv(\"SubtaskA_Trial_Test_Labeled.csv\")\n",
        "valid_data=pd.read_csv(\"SubtaskA_EvaluationData_labeled.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_2TChEuRt53",
        "colab_type": "code",
        "outputId": "1d16cfaf-2179-42ce-8f7e-443222651c3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "# PEEKING INTO THE TRAIN DATA\n",
        "print(train_data.shape)\n",
        "train_data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8500, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>663_3</td>\n",
              "      <td>\"Please enable removing language code from the Dev Center \"language history\" For example if you ever selected \"ru\" and \"ru-ru\" laguages and you published this xap to the Store then it causes Tile localization to show the en-us(default) tile localization which is bad.\"</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>663_4</td>\n",
              "      <td>\"Note: in your .csproj file, there is a SupportedCultures entry like this: &lt;SupportedCultures&gt;de-DE;ru;ru-RU &lt;/SupportedCultures&gt; When I removed the \"ru\" language code and published my new xap version, the old xap version still remains in the Store with \"Replaced and unpublished\".\"</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>664_1</td>\n",
              "      <td>\"Wich means the new version not fully replaced the old version and this causes me very serious problems: 1.\"</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>664_2</td>\n",
              "      <td>\"Some of my users will still receive the old xap version of my app.\"</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>664_3</td>\n",
              "      <td>\"The store randomly gives the old xap or the new xap version of my app.\"</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id  ... label\n",
              "0  663_3  ...     1\n",
              "1  663_4  ...     0\n",
              "2  664_1  ...     0\n",
              "3  664_2  ...     0\n",
              "4  664_3  ...     0\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1pVvsvWRyUW",
        "colab_type": "code",
        "outputId": "a178cf85-617b-42a4-9aa6-d5f1c0274b4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# PEEKING INTO THE TEST DATA\n",
        "print(test_data.shape)\n",
        "test_data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(592, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1310_1</td>\n",
              "      <td>I'm not asking Microsoft to Gives permission like Android so any app can take my data, but don't keep it restricted like iPhone.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1312_1</td>\n",
              "      <td>somewhere between Android and iPhone.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1313_1</td>\n",
              "      <td>And in the Windows Store you can flag the App [Requires Trust] for example.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1313_2</td>\n",
              "      <td>Many thanks Sameh Hi, As we know, there is a lot of limitations is WP8 OS due the high security in the OS itself which is very good, but some time we need to allow some apps to do extra works, apps which we trust i.e: hotmail app, facebook app, skype app ....</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1313_3</td>\n",
              "      <td>The idea is that we can develop a regular app and we request our permissions in the manifest, OR the app can ASK FOR TRUST_�_ more</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       id  ... label\n",
              "0  1310_1  ...     1\n",
              "1  1312_1  ...     0\n",
              "2  1313_1  ...     0\n",
              "3  1313_2  ...     0\n",
              "4  1313_3  ...     1\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhYQJmXTRzcP",
        "colab_type": "code",
        "outputId": "8a8e11dd-c509-4db8-b502-a9147438aeba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "# PEEKING INTO THE VALIDATION DATA\n",
        "print(valid_data.shape)\n",
        "valid_data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(833, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9566</td>\n",
              "      <td>This would enable live traffic aware apps.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9569</td>\n",
              "      <td>Please try other formatting like bold italics shadow to distinguish titles/subtitles from content.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9576</td>\n",
              "      <td>Since computers were invented to save time I suggest we be allowed to upload them all in one zip file - using numbering for the file names and the portal could place them in the right order.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9577</td>\n",
              "      <td>Allow rearranging if the user wants to change them!</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9579</td>\n",
              "      <td>Add SIMD instructions for better use of ARM NEON instructions for math and games.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     id  ... label\n",
              "0  9566  ...     0\n",
              "1  9569  ...     1\n",
              "2  9576  ...     1\n",
              "3  9577  ...     1\n",
              "4  9579  ...     1\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S27MHMdNR0tY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#####################################################################################\n",
        "##########################        CLEANING THE DATA        ##########################\n",
        "#####################################################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lgferb94R2EM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CONTRACTION_MAP = {\"ain't\": \"is not\",\"aren't\": \"are not\",\"can't\": \"cannot\",\"can't've\": \"cannot have\",\"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\"couldn't've\": \"could not have\",\"didn't\": \"did not\",\"doesn't\": \"does not\",\"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\"he'd've\": \"he would have\",\"he'll\": \"he will\",\"he'll've\": \"he he will have\",\"he's\": \"he is\",\"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"how's\": \"how is\",\"I'd\": \"I would\",\"I'd've\": \"I would have\",\"I'll\": \"I will\",\"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\",\"i'd\": \"i would\",\"i'd've\": \"i would have\",\"i'll\": \"i will\",\"i'll've\": \"i will have\",\"i'm\": \"i am\",\"i've\": \"i have\",\"isn't\": \"is not\",\"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\"it'll've\": \"it will have\",\"it's\": \"it is\",\"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\"mustn't've\": \"must not have\",\"needn't\": \"need not\",\"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\"she'll\": \"she will\",\"she'll've\": \"she will have\",\"she's\": \"she is\",\"should've\": \"should have\",\"shouldn't\": \"should not\",\"shouldn't've\": \"should not have\",\"so've\": \"so have\",\"so's\": \"so as\",\"that'd\": \"that would\",\"that'd've\": \"that would have\",\"that's\": \"that is\",\"there'd\": \"there would\",\"there'd've\": \"there would have\",\"there's\": \"there is\",\"they'd\": \"they would\",\"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\"they're\": \"they are\",\"they've\": \"they have\",\"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\"weren't\": \"were not\",\"what'll\": \"what will\",\"what'll've\": \"what will have\",\"what're\": \"what are\",\"what's\": \"what is\",\"what've\": \"what have\",\"when's\": \"when is\",\"when've\": \"when have\",\"where'd\": \"where did\",\"where's\": \"where is\",\"where've\": \"where have\",\"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who's\": \"who is\",\"who've\": \"who have\",\"why's\": \"why is\",\"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\"won't've\": \"will not have\",\"would've\": \"would have\",\"wouldn't\": \"would not\",\"wouldn't've\": \"would not have\",\"y'all\": \"you all\",\"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\",\"you'd've\": \"you would have\",\"you'll\": \"you will\",\"you'll've\": \"you will have\",\"you're\": \"you are\",\"you've\": \"you have\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxYnnfrjR4qt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cleanData(data):\n",
        "\n",
        "    ## REMOVING ASCENTED CHARACTERS LIKE é\n",
        "    def removeAscentedCharacters(text):\n",
        "        text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "        return text\n",
        "    \n",
        "    \n",
        "    ## EXPANDING THE SHORT WORDS:\n",
        "    def expandContractions(text, contraction_mapping=CONTRACTION_MAP):\n",
        "        contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
        "                                          flags=re.IGNORECASE|re.DOTALL)\n",
        "        def expand_match(contraction):\n",
        "            match = contraction.group(0)\n",
        "            first_char = match[0]\n",
        "            expanded_contraction = contraction_mapping.get(match)\\\n",
        "                                    if contraction_mapping.get(match)\\\n",
        "                                    else contraction_mapping.get(match.lower())                       \n",
        "            expanded_contraction = first_char+expanded_contraction[1:]\n",
        "            return expanded_contraction\n",
        "            \n",
        "        expanded_text = contractions_pattern.sub(expand_match, text)\n",
        "        expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
        "        return expanded_text\n",
        "    \n",
        "    ## REMOVING FRONT AND BACK INVERTED COMMAS\n",
        "    def removeIC(text):\n",
        "        if len(text)>=2:\n",
        "          if text[0]=='\"':\n",
        "            text = text[1:]\n",
        "          if text[-1]=='\"':\n",
        "            text = text[:-1]\n",
        "        return text\n",
        "    \n",
        "    ## REMOVING TAGS\n",
        "    def remove_tags(text):\n",
        "        soup = BeautifulSoup(text)\n",
        "        return soup.get_text()\n",
        "\n",
        "    def deEmojify(inputString):\n",
        "        return inputString.encode('ascii', 'ignore').decode('ascii')\n",
        "\n",
        "    def removeSpaces(text):\n",
        "        text= re.sub(' +', ' ', text)\n",
        "        if text[0]==' ':\n",
        "          text=text[1:]\n",
        "        return text\n",
        "\n",
        "    def get_simple_pos(tag):\n",
        "        if tag.startswith('J'):\n",
        "            return wordnet.ADJ\n",
        "        elif tag.startswith('V'):\n",
        "            return wordnet.VERB\n",
        "        elif tag.startswith('N'):\n",
        "            return wordnet.NOUN\n",
        "        elif tag.startswith('R'):\n",
        "            return wordnet.ADV\n",
        "        else:\n",
        "            return wordnet.NOUN  \n",
        "\n",
        "    # OUR STEMMING FUNCTION\n",
        "    def stem(words):\n",
        "      output_words=[]\n",
        "      if len(words)!=0:\n",
        "        words[0] = words[0].lower()\n",
        "      for w in words:\n",
        "              pos=pos_tag([w])\n",
        "              simple_pos = get_simple_pos(pos[0][1])\n",
        "              clean_word=lemmatizer.lemmatize(w,simple_pos)\n",
        "              output_words.append(clean_word.lower())\n",
        "      return output_words\n",
        "\n",
        "    def stemmizeSentence(sentence):\n",
        "      output_words = stem(sentence)\n",
        "      output_wordsf = []\n",
        "      for i in output_words:\n",
        "        if i in corpus_words:\n",
        "          output_wordsf.append(i)\n",
        "      return output_wordsf\n",
        "\n",
        "    print('REMOVING ASCENTED CHARACTERS...')\n",
        "    cleaned = data.apply(lambda x: removeAscentedCharacters(x))\n",
        "    print('NORMALIZING THE SENTENCE CASE...')\n",
        "    cleaned = cleaned.apply(lambda x: x.lower())\n",
        "    print('EXPANDING CONTRACTIONS...')\n",
        "    cleaned = cleaned.apply(lambda x: expandContractions(x))\n",
        "    print('REMOVING IC...')\n",
        "    cleaned=  cleaned.apply(lambda x: removeIC(x))\n",
        "    print('REMOVING TAGS...')\n",
        "    cleaned = cleaned.apply(lambda x: remove_tags(x))\n",
        "    print('REMOVING LINKS...')\n",
        "    cleaned = cleaned.str.replace(\"(https?:\\/\\/)(\\s)*(www\\.)?(\\s)*((\\w|\\s)+\\.)*([\\w\\-\\s]+\\/)*([\\w\\-]+)((\\?)?[\\w\\s]*=\\s*[\\w\\%&]*)*\",\" \")\n",
        "    print('REMOVING SPECIAL CHARACTERS...')\n",
        "    cleaned = cleaned.str.replace(\"\\\".*?\\\"|\\(.*?\\)|<.*>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});|[^a-zA-Z#]\",\" \")\n",
        "    print('REMOVING EMOJIS...')\n",
        "    cleaned = cleaned.apply(lambda x: deEmojify(x))\n",
        "    print('REMOVING UNNECCESSARY SPACES...')\n",
        "    cleaned = cleaned.apply(lambda x: removeSpaces(x))\n",
        "    print('REMOVING THE CONSECUTIVELY REPEATED WORDS...')\n",
        "    cleaned = cleaned.apply(lambda x: re.sub(r'\\b(.+)\\s+\\1\\b', r'\\1', x))\n",
        "    print('REMOVING STOP WORDS...')\n",
        "    tokenized_sentence = cleaned.apply(lambda x: x.split())\n",
        "    tokenized_sentence = tokenized_sentence.apply(lambda sentence: [word for word in sentence if len(word)>2 ])\n",
        "    # tokenized_sentence = tokenized_sentence.apply( lambda sentence: [word for word in sentence if word not in cachedStopWords] )\n",
        "    print('TOKENIZING AND STEMMING...')\n",
        "    tokenized_sentence = tokenized_sentence.apply(lambda sentence: stemmizeSentence(sentence))\n",
        "    print('FINALIZING THE DATA')\n",
        "    detokenized= tokenized_sentence.apply(lambda x: ' '.join(x))\n",
        "\n",
        "    return detokenized"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCrQjj0VR6HC",
        "colab_type": "code",
        "outputId": "38ace792-b8c4-4f51-f538-f7fd52202fb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 736
        }
      },
      "source": [
        "x_train = cleanData(train_data['sentence'])\n",
        "print(x_train.head())\n",
        "print(x_train.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "REMOVING ASCENTED CHARACTERS...\n",
            "NORMALIZING THE SENTENCE CASE...\n",
            "EXPANDING CONTRACTIONS...\n",
            "REMOVING IC...\n",
            "REMOVING TAGS...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"http://windowsphone.uservoice.com/forums/101801-feature-suggestions/suggestions/6080912-we-want-open-the-app-by-tap-the-quick-status-icon\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"http://forums.wpcentral.com/windows-phone-apps/235446-wi-fi-analyzer-possible-wp8.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"http://www.pitorque.de/mistergoodcat/post/somethings-missing-from-the-webbrowser-control.aspx\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"http://wpdev.uservoice.com/forums/110705-app-platform/suggestions/1908267-extend-the-api-to-query-phone-volume-and-vibration\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:273: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
            "  ' Beautiful Soup.' % markup)\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"http://connect.microsoft.com/visualstudio/feedback/details/613932/windows-phone-7-visual-studio-generates-uncompilable-code-when-adding-an-image-to-a-resource-file\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"http://wpdev.uservoice.com/forums/110705-app-platform/suggestions/1908309-enable-microsoft-xna-framework-media-visualization\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"http://yves.vg/testcases/ie_mobi/keyup_event.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"http://mrreaderblog.curioustimes.de/post/52060909928/supported-google-reader-alternatives-part-two\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"http://getpocket.com/developer/docs/authentication\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"http://msdn.microsoft.com/en-us/library/windowsphone/develop/jj681688(v=vs.105).aspx\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "REMOVING LINKS...\n",
            "REMOVING SPECIAL CHARACTERS...\n",
            "REMOVING EMOJIS...\n",
            "REMOVING UNNECCESSARY SPACES...\n",
            "REMOVING THE CONSECUTIVELY REPEATED WORDS...\n",
            "REMOVING STOP WORDS...\n",
            "TOKENIZING AND STEMMING...\n",
            "FINALIZING THE DATA\n",
            "0    please enable remove language code from the dev center for example you ever select and and you publish this the store then cause tile localization show the tile localization which bad\n",
            "1                                                    note your file there entry like this when remove the language code and publish new version the old version still remains the store with\n",
            "2                                                                                                 mean the new version not fully replace the old version and this cause very serious problem\n",
            "3                                                                                                                                               some user will still receive the old version\n",
            "4                                                                                                                                            the store randomly give the old the new version\n",
            "Name: sentence, dtype: object\n",
            "(8500,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuNU45A1R8ol",
        "colab_type": "code",
        "outputId": "53e02996-8425-4866-f8be-f0bf86dea5d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "x_valid = cleanData(valid_data['sentence'])\n",
        "print(x_valid.head())\n",
        "print(x_valid.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "REMOVING ASCENTED CHARACTERS...\n",
            "NORMALIZING THE SENTENCE CASE...\n",
            "EXPANDING CONTRACTIONS...\n",
            "REMOVING IC...\n",
            "REMOVING TAGS...\n",
            "REMOVING LINKS...\n",
            "REMOVING SPECIAL CHARACTERS...\n",
            "REMOVING EMOJIS...\n",
            "REMOVING UNNECCESSARY SPACES...\n",
            "REMOVING THE CONSECUTIVELY REPEATED WORDS...\n",
            "REMOVING STOP WORDS...\n",
            "TOKENIZING AND STEMMING...\n",
            "FINALIZING THE DATA\n",
            "0                                                                                                                   this would enable live traffic aware\n",
            "1                                                                       please try other format like bold shadow distinguish title subtitle from content\n",
            "2    since computer be invent save time suggest allow them all one zip file use number for the file name and the portal could place them the right order\n",
            "3                                                                                                              allow rearrange the user want change them\n",
            "4                                                                                    add instruction for well use arm neon instruction for math and game\n",
            "Name: sentence, dtype: object\n",
            "(833,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNeLmlYDR-4J",
        "colab_type": "code",
        "outputId": "9a6a6a07-bfff-4826-c412-ba19df074374",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "x_test = cleanData(test_data['sentence'])\n",
        "print(x_test.head())\n",
        "print(x_test.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "REMOVING ASCENTED CHARACTERS...\n",
            "NORMALIZING THE SENTENCE CASE...\n",
            "EXPANDING CONTRACTIONS...\n",
            "REMOVING IC...\n",
            "REMOVING TAGS...\n",
            "REMOVING LINKS...\n",
            "REMOVING SPECIAL CHARACTERS...\n",
            "REMOVING EMOJIS...\n",
            "REMOVING UNNECCESSARY SPACES...\n",
            "REMOVING THE CONSECUTIVELY REPEATED WORDS...\n",
            "REMOVING STOP WORDS...\n",
            "TOKENIZING AND STEMMING...\n",
            "FINALIZING THE DATA\n",
            "0                                                              not ask give permission like android any can take data but not keep restrict like\n",
            "1                                                                                                                  somewhere between android and\n",
            "2                                                                                and the window store you can flag the require trust for example\n",
            "3    many thanks know there lot limitation due the high security the itself which very good but some time need allow some extra work which trust\n",
            "4                                           the idea that can develop regular and request our permission the manifest the can ask for trust more\n",
            "Name: sentence, dtype: object\n",
            "(592,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98Cz9NIkSAUj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train=train_data['label']\n",
        "y_valid=valid_data['label']\n",
        "y_test=test_data['label']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFbKAkGlSBiq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#####################################################################################\n",
        "########################## BUILDING THE EMBEDDING MATRIX   ##########################\n",
        "#####################################################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ow39BcMSCwc",
        "colab_type": "code",
        "outputId": "40b83182-7b61-44ae-94a4-64bb92816075",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# BUILDING VOCABULARY FROM THE SENTENCES\n",
        "# THIS WILL HELP IN GETTING THE INPUT SEQUENCES FOR THE \n",
        "mxlen=0\n",
        "tokenized=x_train.apply(lambda x: x.split())\n",
        "for tokens in tokenized:\n",
        "  mxlen=max(mxlen,len(tokens))\n",
        "print('MAX LEN', mxlen)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAX LEN 126\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDKrwthMSETX",
        "colab_type": "code",
        "outputId": "85d0f66b-4dfb-490e-a19a-f10e2e39caea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "all_words= [ word for tokens in tokenized for word in tokens]\n",
        "vocab = sorted(list(set(all_words)))\n",
        "vocab_train_len=len(vocab)\n",
        "print('VOCAB SIZE',len(vocab))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VOCAB SIZE 3819\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XfegBc5SFgT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MAX LEN OF AN INPUT SEQUENCE\n",
        "MXSEQLEN=126\n",
        "# GOOGLE NEWS WORD VECTOR ENCODING SIZE\n",
        "MAX_NB_WORDS = 4000\n",
        "EMBEDDING_DIM = 300"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOMCY2hiSGod",
        "colab_type": "code",
        "outputId": "0993809c-fc7f-4a98-bc32-2beb6435e4ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# BUILDING TOKENIZER FROM THE TRAINING DATA\n",
        "tokenizer = Tokenizer(num_words=vocab_train_len, lower=True, char_level=False)\n",
        "tokenizer.fit_on_texts(x_train.tolist())\n",
        "print('Found %s unique tokens.' % len(tokenizer.word_index))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3819 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sqpos2MRSIAn",
        "colab_type": "code",
        "outputId": "9f7e8391-c5c0-4c9c-8e3d-6e1a8d911efd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        }
      },
      "source": [
        "# form the sequences that will be the input to the network\n",
        "# padd or remove values to make sequences of equal length\n",
        "train_word_index= tokenizer.word_index\n",
        "train_sequence = tokenizer.texts_to_sequences(x_train.tolist())\n",
        "train_sequence = sequence.pad_sequences(train_sequence, maxlen=MXSEQLEN)\n",
        "print(train_sequence)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   0    0    0 ...  804   37  341]\n",
            " [   0    0    0 ...    1   35    9]\n",
            " [   0    0    0 ...   78  805   84]\n",
            " ...\n",
            " [   0    0    0 ...   37 1192   12]\n",
            " [   0    0    0 ...  530  740   18]\n",
            " [   0    0    0 ...    2   29  137]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q88TKoUHSKH5",
        "colab_type": "code",
        "outputId": "b7ced427-a92b-4eca-d8ff-dec857b8cbca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        }
      },
      "source": [
        "# TEST SEQUENCE BUILT FROM THE SAME TRAINING VOCABULARY\n",
        "test_sequence = tokenizer.texts_to_sequences(x_test.tolist())\n",
        "test_sequence = sequence.pad_sequences(test_sequence, maxlen=MXSEQLEN)\n",
        "print(test_sequence)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   0    0    0 ...  279 1092   17]\n",
            " [   0    0    0 ...  237  125    2]\n",
            " [   0    0    0 ...  731    3   85]\n",
            " ...\n",
            " [   0    0    0 ...  342    3  236]\n",
            " [   0    0    0 ...    1  264  272]\n",
            " [   0    0    0 ...  175    1  351]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7f_q5fnSLpE",
        "colab_type": "code",
        "outputId": "49244249-06df-4fe9-841c-75c18b6e36d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        }
      },
      "source": [
        "# TEST SEQUENCE BUILT FROM THE SAME TRAINING VOCABULARY\n",
        "valid_sequence = tokenizer.texts_to_sequences(x_valid.tolist())\n",
        "valid_sequence = sequence.pad_sequences(valid_sequence, maxlen=MXSEQLEN)\n",
        "print(valid_sequence)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   0    0    0 ...  292 1624 1363]\n",
            " [   0    0    0 ... 1362   19  131]\n",
            " [   0    0    0 ...    1  106  383]\n",
            " ...\n",
            " [   0    0    0 ...   11   78  823]\n",
            " [   0    0    0 ...    6   60   83]\n",
            " [   0    0    0 ...    8  190  844]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXTZKYeFSNNI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DOWNLOADING THE PRETRAINED FAST TEXT WORD TO VECTOR REPRESENTATIONS\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
        "!unzip wiki-news-300d-1M.vec.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUeoE7IxSUR1",
        "colab_type": "code",
        "outputId": "72747a13-2f47-4763-ac07-d613948b9518",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "print('loading word embeddings...')\n",
        "embeddings_index = {}\n",
        "f = codecs.open('./wiki-news-300d-1M.vec', encoding='utf-8')\n",
        "for line in tqdm(f):\n",
        "    values = line.rstrip().rsplit(' ')\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('found %s word vectors' % len(embeddings_index))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1080it [00:00, 10799.42it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loading word embeddings...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "999995it [01:42, 9763.85it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "found 999995 word vectors\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WukOiHCwSfjL",
        "colab_type": "code",
        "outputId": "ff141f97-5d24-489f-e6b8-e5e1206ea3d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#embedding matrix\n",
        "print('preparing embedding matrix...')\n",
        "words_not_found = []\n",
        "nb_words = min(MAX_NB_WORDS, len(train_word_index)+1)\n",
        "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
        "for word, i in train_word_index.items():\n",
        "    if i >= nb_words:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    else:\n",
        "        words_not_found.append(word)\n",
        "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "preparing embedding matrix...\n",
            "number of null word embeddings: 9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRRv2cPvSh9n",
        "colab_type": "code",
        "outputId": "c85b878e-f057-4fbe-89fb-780234ee69a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "print(\"sample words not found: \", np.random.choice(words_not_found, 10))\n",
        "print(embedding_matrix.shape)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample words not found:  ['actable' 'unabstract' 'avahi' 'feedy' 'actable' 'unabstract'\n",
            " 'semitransparency' 'actable' 'wup' 'wup']\n",
            "(3820, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9XEj9hRSnW1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "variables = {\n",
        "    'train_sequence' : train_sequence,\n",
        "    'test_sequence' : test_sequence,\n",
        "    'valid_sequence' :valid_sequence,\n",
        "    'y_train':y_train,\n",
        "    'y_test':y_test,\n",
        "    'y_valid':y_valid,\n",
        "    'train_embedding_weights':embedding_matrix,\n",
        "    'EMBEDDING_DIM':EMBEDDING_DIM,\n",
        "    'MXSEQLEN':MXSEQLEN,\n",
        "    'train_word_index':train_word_index\n",
        "}\n",
        "\n",
        "name = 'variablesFastText' + str(MXSEQLEN)\n",
        "pickle.dump(variables,open(name,'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GEfFwR6nrO6W"
   },
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "######################### IMPORTING THE LIBRARIES AND DATASET #######################\n",
    "#####################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G1IjTy1ovP4G"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# FOR PLOTTING GRAPHS\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_colwidth',300)\n",
    "\n",
    "# FOR REMOVING SPECIAL CHARACTERS, LINKS, AND EXPANDING WORDS\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# FOR STEMMING AND REMOVING STOP WORDS\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer   \n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "\n",
    "# FOR BUILDING THE EMBEDDING MATRIX AND GENERATING THE SEQUENCES\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "# FOR THE GOOGLE WORD TO VECTOR WEIGHTS\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hCJ5cu5Fr9Zb"
   },
   "outputs": [],
   "source": [
    "# IMPORTING THE DATASET\n",
    "train_data=pd.read_csv(\"../TrainingData.csv\")\n",
    "test_data=pd.read_csv(\"../SubtaskA_Trial_Test_Labeled.csv\")\n",
    "valid_data=pd.read_csv(\"../SubtaskA_EvaluationData_labeled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "M3-8OLPSw1EC",
    "outputId": "3d9ddb21-01d3-45ff-b589-a339d3a8cb89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8500, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>663_3</td>\n",
       "      <td>\"Please enable removing language code from the Dev Center \"language history\" For example if you ever selected \"ru\" and \"ru-ru\" laguages and you published this xap to the Store then it causes Tile localization to show the en-us(default) tile localization which is bad.\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>663_4</td>\n",
       "      <td>\"Note: in your .csproj file, there is a SupportedCultures entry like this: &lt;SupportedCultures&gt;de-DE;ru;ru-RU &lt;/SupportedCultures&gt; When I removed the \"ru\" language code and published my new xap version, the old xap version still remains in the Store with \"Replaced and unpublished\".\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>664_1</td>\n",
       "      <td>\"Wich means the new version not fully replaced the old version and this causes me very serious problems: 1.\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>664_2</td>\n",
       "      <td>\"Some of my users will still receive the old xap version of my app.\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>664_3</td>\n",
       "      <td>\"The store randomly gives the old xap or the new xap version of my app.\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  ... label\n",
       "0  663_3  ...     1\n",
       "1  663_4  ...     0\n",
       "2  664_1  ...     0\n",
       "3  664_2  ...     0\n",
       "4  664_3  ...     0\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PEEKING INTO THE TRAIN DATA\n",
    "print(train_data.shape)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "5VQq5pN-xNED",
    "outputId": "af8b3f81-58fd-4ff6-a760-cad28b7ecae0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(592, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1310_1</td>\n",
       "      <td>I'm not asking Microsoft to Gives permission like Android so any app can take my data, but don't keep it restricted like iPhone.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1312_1</td>\n",
       "      <td>somewhere between Android and iPhone.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1313_1</td>\n",
       "      <td>And in the Windows Store you can flag the App [Requires Trust] for example.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1313_2</td>\n",
       "      <td>Many thanks Sameh Hi, As we know, there is a lot of limitations is WP8 OS due the high security in the OS itself which is very good, but some time we need to allow some apps to do extra works, apps which we trust i.e: hotmail app, facebook app, skype app ....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1313_3</td>\n",
       "      <td>The idea is that we can develop a regular app and we request our permissions in the manifest, OR the app can ASK FOR TRUST_�_ more</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  ... label\n",
       "0  1310_1  ...     1\n",
       "1  1312_1  ...     0\n",
       "2  1313_1  ...     0\n",
       "3  1313_2  ...     0\n",
       "4  1313_3  ...     1\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PEEKING INTO THE TEST DATA\n",
    "print(test_data.shape)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "-WNTplTuxMEK",
    "outputId": "78124f6c-faf2-4c8f-bc14-49e4752101b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(833, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9566</td>\n",
       "      <td>This would enable live traffic aware apps.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9569</td>\n",
       "      <td>Please try other formatting like bold italics shadow to distinguish titles/subtitles from content.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9576</td>\n",
       "      <td>Since computers were invented to save time I suggest we be allowed to upload them all in one zip file - using numbering for the file names and the portal could place them in the right order.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9577</td>\n",
       "      <td>Allow rearranging if the user wants to change them!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9579</td>\n",
       "      <td>Add SIMD instructions for better use of ARM NEON instructions for math and games.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  ... label\n",
       "0  9566  ...     0\n",
       "1  9569  ...     1\n",
       "2  9576  ...     1\n",
       "3  9577  ...     1\n",
       "4  9579  ...     1\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PEEKING INTO THE VALIDATION DATA\n",
    "print(valid_data.shape)\n",
    "valid_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wVvBJmIsvTc8"
   },
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "##########################        CLEANING THE DATA        ##########################\n",
    "#####################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d22xmXOe23gH"
   },
   "outputs": [],
   "source": [
    "CONTRACTION_MAP = {\"ain't\": \"is not\",\"aren't\": \"are not\",\"can't\": \"cannot\",\"can't've\": \"cannot have\",\"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\"couldn't've\": \"could not have\",\"didn't\": \"did not\",\"doesn't\": \"does not\",\"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\"he'd've\": \"he would have\",\"he'll\": \"he will\",\"he'll've\": \"he he will have\",\"he's\": \"he is\",\"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"how's\": \"how is\",\"I'd\": \"I would\",\"I'd've\": \"I would have\",\"I'll\": \"I will\",\"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\",\"i'd\": \"i would\",\"i'd've\": \"i would have\",\"i'll\": \"i will\",\"i'll've\": \"i will have\",\"i'm\": \"i am\",\"i've\": \"i have\",\"isn't\": \"is not\",\"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\"it'll've\": \"it will have\",\"it's\": \"it is\",\"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\"mustn't've\": \"must not have\",\"needn't\": \"need not\",\"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\"she'll\": \"she will\",\"she'll've\": \"she will have\",\"she's\": \"she is\",\"should've\": \"should have\",\"shouldn't\": \"should not\",\"shouldn't've\": \"should not have\",\"so've\": \"so have\",\"so's\": \"so as\",\"that'd\": \"that would\",\"that'd've\": \"that would have\",\"that's\": \"that is\",\"there'd\": \"there would\",\"there'd've\": \"there would have\",\"there's\": \"there is\",\"they'd\": \"they would\",\"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\"they're\": \"they are\",\"they've\": \"they have\",\"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\"weren't\": \"were not\",\"what'll\": \"what will\",\"what'll've\": \"what will have\",\"what're\": \"what are\",\"what's\": \"what is\",\"what've\": \"what have\",\"when's\": \"when is\",\"when've\": \"when have\",\"where'd\": \"where did\",\"where's\": \"where is\",\"where've\": \"where have\",\"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who's\": \"who is\",\"who've\": \"who have\",\"why's\": \"why is\",\"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\"won't've\": \"will not have\",\"would've\": \"would have\",\"wouldn't\": \"would not\",\"wouldn't've\": \"would not have\",\"y'all\": \"you all\",\"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\",\"you'd've\": \"you would have\",\"you'll\": \"you will\",\"you'll've\": \"you will have\",\"you're\": \"you are\",\"you've\": \"you have\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "eIl8oO-iCtI1",
    "outputId": "04f85078-9686-405d-e6fc-3fc01741c68c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to /root/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('stopwords')\n",
    "#### CACHING THE STOP WORDS HELPS IN FASTENING THE REMOVAL OF THE STOP WORDS\n",
    "# cachedStopWords = stopwords.words(\"english\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "corpus_words = set(nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D-yiAfpTva1j"
   },
   "outputs": [],
   "source": [
    "def cleanData(data):\n",
    "\n",
    "    ## REMOVING ASCENTED CHARACTERS LIKE é\n",
    "    def removeAscentedCharacters(text):\n",
    "        text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        return text\n",
    "\n",
    "    ## EXPANDING THE SHORT WORDS:\n",
    "    def expandContractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "        contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                          flags=re.IGNORECASE|re.DOTALL)\n",
    "        def expand_match(contraction):\n",
    "            match = contraction.group(0)\n",
    "            first_char = match[0]\n",
    "            expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                    if contraction_mapping.get(match)\\\n",
    "                                    else contraction_mapping.get(match.lower())                       \n",
    "            expanded_contraction = first_char+expanded_contraction[1:]\n",
    "            return expanded_contraction\n",
    "            \n",
    "        expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "        expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "        return expanded_text\n",
    "\n",
    "    ## REMOVING INVERTED COMMAS\n",
    "    def removeIC(text):\n",
    "        if len(text)>=2:\n",
    "          if text[0]=='\"':\n",
    "            text = text[1:]\n",
    "          if text[-1]=='\"':\n",
    "            text = text[:-1]\n",
    "        return text\n",
    "\n",
    "    def get_simple_pos(tag):\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN  \n",
    "\n",
    "    # OUR STEMMING FUNCTION\n",
    "    def stem(words):\n",
    "      output_words=[]\n",
    "      if len(words)!=0:\n",
    "        words[0] = words[0].lower()\n",
    "      for w in words:\n",
    "              pos=pos_tag([w])\n",
    "              simple_pos = get_simple_pos(pos[0][1])\n",
    "              clean_word=lemmatizer.lemmatize(w,simple_pos)\n",
    "              output_words.append(clean_word.lower())\n",
    "      return output_words\n",
    "\n",
    "    def stemmizeSentence(sentence):\n",
    "      output_words = stem(sentence)\n",
    "      output_wordsf = []\n",
    "      for i in output_words:\n",
    "        if i in corpus_words:\n",
    "          output_wordsf.append(i)\n",
    "      return output_wordsf\n",
    "\n",
    "    print('EXPANDING CONTRACTIONS AND REMOVING ASCENTED CHARACTERS...')\n",
    "    data['clean'] = [removeIC(expandContractions(removeAscentedCharacters(i.lower()))) for i in data['sentence']]\n",
    "\n",
    "    print('REMOVING LINKS AND SPECIAL CHARACTERS...')\n",
    "    data['clean'] = data['clean'].str.replace(\"(https?:\\/\\/)(\\s)*(www\\.)?(\\s)*((\\w|\\s)+\\.)*([\\w\\-\\s]+\\/)*([\\w\\-]+)((\\?)?[\\w\\s]*=\\s*[\\w\\%&]*)*\",\" \")\n",
    "    data['clean'] = data['clean'].str.replace(\"\\\".*?\\\"|\\(.*?\\)|<.*>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});|[^a-zA-Z#]\",\" \")\n",
    "\n",
    "    print('REMOVING STOP WORDS...')\n",
    "    tokenized_sentence = data['clean'].apply(lambda x: x.split())\n",
    "    tokenized_sentence = tokenized_sentence.apply(lambda sentence: [word for word in sentence if len(word)>2 ])\n",
    "    # tokenized_sentence = tokenized_sentence.apply( lambda sentence: [word for word in sentence if word not in cachedStopWords] )\n",
    "\n",
    "    print('APPLYING STEMMING...')\n",
    "    tokenized_sentence = tokenized_sentence.apply(lambda sentence: stemmizeSentence(sentence))\n",
    "    # DETOKENIZING THE TOKENS BACK TO THE SENTENCES\n",
    "    detokenized= tokenized_sentence.apply(lambda x: ' '.join(x))\n",
    "\n",
    "    return detokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "ovviPnapwRq6",
    "outputId": "8a5c5616-2301-4c8c-987e-b0f03766efc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPANDING CONTRACTIONS AND REMOVING ASCENTED CHARACTERS...\n",
      "REMOVING LINKS AND SPECIAL CHARACTERS...\n",
      "REMOVING STOP WORDS...\n",
      "APPLYING STEMMING...\n",
      "0    please enable remove language code from the dev center for example you ever select and and you publish this the store then cause tile localization show the tile localization which bad\n",
      "1                                                    note your file there entry like this when remove the language code and publish new version the old version still remains the store with\n",
      "2                                                                                                 mean the new version not fully replace the old version and this cause very serious problem\n",
      "3                                                                                                                                               some user will still receive the old version\n",
      "4                                                                                                                                            the store randomly give the old the new version\n",
      "Name: clean, dtype: object\n",
      "(8500,)\n"
     ]
    }
   ],
   "source": [
    "x_train = cleanData(train_data)\n",
    "print(x_train.head())\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "aE32ORTghQ3O",
    "outputId": "24826f22-ae71-4f37-bbc4-d08226f38170"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPANDING CONTRACTIONS AND REMOVING ASCENTED CHARACTERS...\n",
      "REMOVING LINKS AND SPECIAL CHARACTERS...\n",
      "REMOVING STOP WORDS...\n",
      "APPLYING STEMMING...\n",
      "0                                                                                                                   this would enable live traffic aware\n",
      "1                                                                       please try other format like bold shadow distinguish title subtitle from content\n",
      "2    since computer be invent save time suggest allow them all one zip file use number for the file name and the portal could place them the right order\n",
      "3                                                                                                              allow rearrange the user want change them\n",
      "4                                                                                    add instruction for well use arm neon instruction for math and game\n",
      "Name: clean, dtype: object\n",
      "(833,)\n"
     ]
    }
   ],
   "source": [
    "x_valid = cleanData(valid_data)\n",
    "print(x_valid.head())\n",
    "print(x_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "T9gPemz-hUnO",
    "outputId": "25c8bbcc-d66a-4e3c-9592-41fe58ec673d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPANDING CONTRACTIONS AND REMOVING ASCENTED CHARACTERS...\n",
      "REMOVING LINKS AND SPECIAL CHARACTERS...\n",
      "REMOVING STOP WORDS...\n",
      "APPLYING STEMMING...\n",
      "0                                                              not ask give permission like android any can take data but not keep restrict like\n",
      "1                                                                                                                  somewhere between android and\n",
      "2                                                                                and the window store you can flag the require trust for example\n",
      "3    many thanks know there lot limitation due the high security the itself which very good but some time need allow some extra work which trust\n",
      "4                                           the idea that can develop regular and request our permission the manifest the can ask for trust more\n",
      "Name: clean, dtype: object\n",
      "(592,)\n"
     ]
    }
   ],
   "source": [
    "x_test = cleanData(test_data)\n",
    "print(x_test.head())\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9s_8Gm2a0HKq"
   },
   "outputs": [],
   "source": [
    "y_train=train_data['label']\n",
    "y_valid=valid_data['label']\n",
    "y_test=test_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NEKgzPGLhvqM"
   },
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "########################## BUILDING THE EMBEDDING MATRIX   ##########################\n",
    "#####################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cLxXazPsiLBL",
    "outputId": "5f85d9c0-fb0e-4084-a1f6-3fe5d1f69640"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX LEN 130\n"
     ]
    }
   ],
   "source": [
    "# BUILDING VOCABULARY FROM THE SENTENCES\n",
    "# THIS WILL HELP IN GETTING THE INPUT SEQUENCES FOR THE \n",
    "mxlen=0\n",
    "tokenized=x_train.apply(lambda x: x.split())\n",
    "for tokens in tokenized:\n",
    "  mxlen=max(mxlen,len(tokens))\n",
    "print('MAX LEN', mxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sWxkEZjWiNLa",
    "outputId": "ba3e3b74-6ebe-47f4-b028-99a329b6ecb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE 3818\n"
     ]
    }
   ],
   "source": [
    "all_words= [ word for tokens in tokenized for word in tokens]\n",
    "vocab = sorted(list(set(all_words)))\n",
    "vocab_train_len=len(vocab)\n",
    "print('VOCAB SIZE',len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qVwZsHuBintp"
   },
   "outputs": [],
   "source": [
    "# MAX LEN OF AN INPUT SEQUENCE\n",
    "MXSEQLEN=80\n",
    "# GOOGLE NEWS WORD VECTOR ENCODING SIZE\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "loKQSopHirnS",
    "outputId": "4f978529-3060-4e33-af8f-32952d105bd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3818 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# BUILDING TOKENIZER FROM THE TRAINING DATA\n",
    "tokenizer = Tokenizer(num_words=vocab_train_len, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(x_train.tolist())\n",
    "print('Found %s unique tokens.' % len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "g7cWSPrgjgic",
    "outputId": "b8cb6317-e819-4e8d-c029-c0557abfedb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...  807   37  342]\n",
      " [   0    0    0 ...    1   35    9]\n",
      " [   0    0    0 ...   77  808   84]\n",
      " ...\n",
      " [   0    0    0 ...   37 1188   12]\n",
      " [   0    0    0 ...  531  741   18]\n",
      " [   0    0    0 ...    2   29  137]]\n"
     ]
    }
   ],
   "source": [
    "# form the sequences that will be the input to the network\n",
    "# padd or remove values to make sequences of equal length\n",
    "train_word_index= tokenizer.word_index\n",
    "train_sequence = tokenizer.texts_to_sequences(x_train.tolist())\n",
    "train_sequence = sequence.pad_sequences(train_sequence, maxlen=MXSEQLEN)\n",
    "print(train_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "fU3Yxy6rj3mO",
    "outputId": "fb51ab43-914e-4ae6-b953-3faf2f96761e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...  279 1091   17]\n",
      " [   0    0    0 ...  237  125    2]\n",
      " [   0    0    0 ...  731    3   85]\n",
      " ...\n",
      " [   0    0    0 ...  343    3  236]\n",
      " [   0    0    0 ...    1  262  271]\n",
      " [   0    0    0 ...  182    1  352]]\n"
     ]
    }
   ],
   "source": [
    "# TEST SEQUENCE BUILT FROM THE SAME TRAINING VOCABULARY\n",
    "test_sequence = tokenizer.texts_to_sequences(x_test.tolist())\n",
    "test_sequence = sequence.pad_sequences(test_sequence, maxlen=MXSEQLEN)\n",
    "print(test_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "r-KdOEjWpifZ",
    "outputId": "4e18891a-c2bb-4cc8-e6a5-c832e2815eef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...  292 1622 1363]\n",
      " [   0    0    0 ... 1362   20  134]\n",
      " [   0    0    0 ...    1  107  384]\n",
      " ...\n",
      " [   0    0    0 ...   77   77  825]\n",
      " [   0    0    0 ...    6   60   82]\n",
      " [   0    0    0 ...    8  190  845]]\n"
     ]
    }
   ],
   "source": [
    "# TEST SEQUENCE BUILT FROM THE SAME TRAINING VOCABULARY\n",
    "valid_sequence = tokenizer.texts_to_sequences(x_valid.tolist())\n",
    "valid_sequence = sequence.pad_sequences(valid_sequence, maxlen=MXSEQLEN)\n",
    "print(valid_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "bUdEfZv-pu0d",
    "outputId": "d0955c68-ba7f-4850-a747-60815affa089"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-04-02 20:48:30--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.146.29\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.146.29|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1647046227 (1.5G) [application/x-gzip]\n",
      "Saving to: ‘/content/GoogleNews-vectors-negative300.bin.gz’\n",
      "\n",
      "GoogleNews-vectors- 100%[===================>]   1.53G  16.6MB/s    in 2m 10s  \n",
      "\n",
      "2020-04-02 20:50:41 (12.1 MB/s) - ‘/content/GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DOWNLOADING THE PRETRAINED GOOGLE NEWS WORD TO VECTOR REPRESENTATIONS\n",
    "!wget -P /content/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
    "!gzip -d /content/GoogleNews-vectors-negative300.bin.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "0N9pcvINpzn4",
    "outputId": "1c874fa4-0f1d-4283-a88b-1ed69eb8d6f7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:410: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "# LOADING WORD TO VECTOR\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format('/content/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "delsHDbzp1oP",
    "outputId": "cd982ad6-dbe3-4284-de3a-5bb952ce9df4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3819, 300)\n"
     ]
    }
   ],
   "source": [
    "# BUILDING THE EMBEDDING WEIGHT MATRIX, \n",
    "# WILL ACT AS NON TRAINABLE EMBEDDING LAYER PARAMETERS FOR THE RNN MODEL\n",
    "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
    "\n",
    "for word,index in train_word_index.items():\n",
    "    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "\n",
    "print(train_embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nZKrBsNQ1CB6",
    "outputId": "579655d5-23c7-493e-db13-d2c4b6e83b5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 61\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "variables = {\n",
    "    'train_sequence' : train_sequence,\n",
    "    'test_sequence' : test_sequence,\n",
    "    'valid_sequence' :valid_sequence,\n",
    "    'y_train':y_train,\n",
    "    'y_test':y_test,\n",
    "    'y_valid':y_valid,\n",
    "    'train_embedding_weights':train_embedding_weights,\n",
    "    'EMBEDDING_DIM':EMBEDDING_DIM,\n",
    "    'MXSEQLEN':MXSEQLEN,\n",
    "    'train_word_index':train_word_index\n",
    "}\n",
    "\n",
    "pickle.dump(variables,open('./Data/variables','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m30yy6l14IYo"
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# variables = pickle.load(open('./Data/variables','rb'))\n",
    "\n",
    "# test_sequence  = variables['test_sequence']\n",
    "# valid_sequence  = variables['valid_sequence']\n",
    "# train_sequence  = variables['train_sequence']\n",
    "\n",
    "# y_train  = variables['y_train']\n",
    "# y_test  = variables['y_test']\n",
    "# y_valid  = variables['y_valid']\n",
    "\n",
    "# MXSEQLEN  = variables['MXSEQLEN']\n",
    "# EMBEDDING_DIM  = variables['EMBEDDING_DIM']\n",
    "# train_embedding_weights  = variables['train_embedding_weights']\n",
    "# train_word_index  = variables['train_word_index']"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SequenceGenerator.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
